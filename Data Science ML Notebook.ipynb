{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in sklearn: Data Exploration and Model Fitting\n",
    "\n",
    "This notebook demonstrates a potential workflow for data exploration and Machine Learning model creation in Python using *sklearn*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "This section contains several methods for the descriptive statistical analysis of datasets.\n",
    "Purpose: exploration and preparation of data for modeling.\n",
    "\n",
    "The script loads the Boston House Prices dataset from the *sklearn* package per default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load demo dataset\n",
    "from sklearn.datasets import load_boston\n",
    "dataset = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First check of data\n",
    "*Print dataset description (only if available):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(dataset, 'DESCR'):\n",
    "    print(dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Convert dataset to Pandas DataFrame (DF), add target data as new column, and check DF head:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data=dataset['data'])\n",
    "if hasattr(dataset, 'feature_names'):\n",
    "    data_df.columns=dataset['feature_names']\n",
    "else:\n",
    "    print(\"Please specify the feature names manually!\")\n",
    "data_df['TARGET'] = dataset['target']\n",
    "print(data_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Print DF statistics:*\n",
    "- distribution of data\n",
    "- sample count\n",
    "- type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Count null values per feature:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count null values per feature:\n",
    "print(data_df.isnull().sum())\n",
    "\n",
    "# drop null values if present:\n",
    "unprocessed_length = len(data_df)\n",
    "data_df = data_df.dropna()\n",
    "processed_length = len(data_df)\n",
    "print(f\"\\nUnprocessed DF length: {unprocessed_length}, processed DF length: {processed_length}, dropped instances: {unprocessed_length - processed_length}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Correlation/distribution preview:*\n",
    "\n",
    "Plot Scatter Matrix of Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "pd.plotting.scatter_matrix(data_df, figsize=((14, 12)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create correlation matrix and plot it as a heatmap:*\n",
    "- highly (positively or negatively) correlated features may contain redundant information - potential targets for removal for more efficient modeling / predictions\n",
    "- desired: features with a high correlation to the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data_df.corr()\n",
    "fig = plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr, square=True, annot=True)  # switch off annot to hide values\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot distribution information:*\n",
    "\n",
    "Issues with the distribution may lead to problems during model fitting. This may be compensated by data transformation and/or under-/oversampling of data.\n",
    "\n",
    "- a) box and whisker plot\n",
    "- b) histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) box and whisker plot\n",
    "sns.set()  # use Seaborn Standard Formatting from here on\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "data_df.plot(kind='box', logy=True, figsize=(14,20), ax=ax[0], title='log. values')\n",
    "data_df.plot(kind='box', logy=False, figsize=(14,20), ax=ax[1], title='values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) histograms\n",
    "fig = plt.figure()\n",
    "data_df.hist(figsize=(14, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*No actions will be performed on the dataset in this demonstration.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing, model creation, and model fit\n",
    "\n",
    "The following steps create the actual model and fit it to the data. The workflow assumes that the target is numeric, i.e., Regressor Models will be used for the fit.\n",
    "\n",
    "Two models will be used:\n",
    "- Random Forest Regressor (i.e., an ensemble of multiple Decision Trees)\n",
    "- K-nearest Neighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scikit learn packages:\n",
    "# data preprocessing:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "# models:\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Shuffle the DataFrame and split it into 20% Test Set and 80% Training Set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = shuffle(data_df)\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.2)\n",
    "\n",
    "# split dataframes into features and targets:\n",
    "if hasattr(dataset, 'feature_names'):\n",
    "    train_data = train_df[dataset['feature_names']]\n",
    "    test_data = test_df[dataset['feature_names']]\n",
    "else:\n",
    "    print(\"Please provide the feature names manually!\")\n",
    "    \n",
    "train_target = train_df['TARGET']\n",
    "test_target = test_df['TARGET']\n",
    "\n",
    "# create dictionary for the results of fitting and scoring:\n",
    "res_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Creation of and fit to the Random Forest Regressor:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# screening parameters - in this demonstration, the number of estimators and the tree depth will be varied:\n",
    "n_estimators = [20, 50, 100]\n",
    "max_depths = [2, 5, 10, 20]\n",
    "\n",
    "for n_est in n_estimators:\n",
    "    for max_dep in max_depths:\n",
    "        current_identifier = f\"RFR model (n_estimators: {n_est}, max_depth: {max_dep})\"\n",
    "        RFR_model = RFR(n_estimators=n_est, max_depth=max_dep)\n",
    "        RFR_model.fit(train_data, train_target)\n",
    "        train_score = RFR_model.score(train_data, train_target)\n",
    "        test_score = RFR_model.score(test_data, test_target)\n",
    "        res_dict[current_identifier] = {'model': RFR_model, 'train score': train_score, 'test score': test_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Creation of and fit to the K-nearest Neighbors Regressor:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# screening parameters - in this demonstration, only the number of neighbors will be varied:\n",
    "n_neighbors = [2, 3, 5, 10]\n",
    "\n",
    "for n_n in n_neighbors:\n",
    "    current_identifier = f\"KNR model (n_neighbors: {n_n})\"\n",
    "    KNR_model = KNR(n_neighbors=n_n)\n",
    "    KNR_model.fit(train_data, train_target)\n",
    "    train_score = KNR_model.score(train_data, train_target)\n",
    "    test_score = KNR_model.score(test_data, test_target)\n",
    "    res_dict[current_identifier] = {'model': KNR_model, 'train score': train_score, 'test score': test_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Iterate through results dictionary and create a results DataFrame:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(columns=['Model', 'Train Score', 'Test Score'])\n",
    "\n",
    "for key, value in res_dict.items():\n",
    "    res_df = res_df.append({'Model': key,\n",
    "                            'Test Score': round(res_dict[key]['test score'], 2),\n",
    "                            'Train Score': round(res_dict[key]['train score'], 2)}, ignore_index=True)\n",
    "\n",
    "# sort values by Test Score in descending order:\n",
    "res_df.sort_values(by='Test Score', ascending=False, inplace=True)\n",
    "res_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print results:\n",
    "print(res_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance:\n",
    "\n",
    "*Create a plot of the Feature Importance based on the best-performing Random Forest Regressor:*\n",
    "\n",
    "The Feature Importance denotes the proportion of the Target determined by the respective Feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate identifier of best-performing Random Forest Regressor model:\n",
    "for ind, row in res_df.iterrows():\n",
    "    if 'RFR model' in row['Model']:\n",
    "        identifier = row['Model']\n",
    "        break\n",
    "\n",
    "# isolate feature importances from model and create dictionary based on these values:\n",
    "feature_imp_dict = dict(zip(dataset['feature_names'], res_dict[identifier]['model'].feature_importances_))\n",
    "# convert dictionary to DataFrame:\n",
    "imp_df = pd.DataFrame.from_dict(data=feature_imp_dict, orient='index')\n",
    "imp_df.columns = ['Feature Importance']\n",
    "\n",
    "# sort values in descending order:\n",
    "imp_df.sort_values(by='Feature Importance', inplace=True, ascending=False)\n",
    "\n",
    "# create plot:\n",
    "fig = plt.figure()\n",
    "imp_df.plot.bar(figsize=(12, 10))\n",
    "plt.ylabel('Relative Feature Importance')\n",
    "plt.xlabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "# print Feature Importance DataFrame:\n",
    "print(\"DataFrame of Feature Importance:\\n\\n\", imp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the best-performing model for predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and print model properties:\n",
    "best_model = res_dict[res_df.iloc[0]['Model']]['model']\n",
    "print(f\"Best-performing model: {res_df.iloc[0]['Model']}.\\nModel properties:\\n {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the best-performing model for predictions:\n",
    "# use first DF row as demo test dataset; replace this row by actual data for real predictions\n",
    "test_dataset = np.array(data_df.iloc[0][:-1].values.reshape(1, -1))  \n",
    "prediction = best_model.predict(test_dataset)  # provide data in form of an array\n",
    "print(f\"Features:\\n{test_dataset}\\n\\n Model prediction: {prediction[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
